{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting from an HMM in split data\n",
    "\n",
    "In certain cases, you may not want to use the built-in cross-validation (CV) functionality (covered in [*Predicting from an HMM*](./Prediction_example.ipynb)). This may be because you are using a different scheme to define your folds, you want to train and test on separate populations (like patients and controls), or you are using training/testing as separate steps in another routine. This notebook shows how to use the toolbox's functionality to do this. \n",
    "\n",
    "If you are new to the (GL)HMM or are unsure how to fit the (GL)HMM, start with the tutorials [*Standard Gaussian Hidden Markov Model*](./GaussianHMM_example.ipynb) or [*Gaussian-Linear Hidden Markov Model*](./GLHMM_example.ipynb). \n",
    "\n",
    "**NOTE: Running this tutorial requires data from the Human Connectome Project (HCP). Make sure you have been registered to use the HCP data. If not, register and download the HCP data by following the instructions on the [Human Connectome Project's website](https://www.humanconnectome.org/study/hcp-young-adult/data-use-terms).**\n",
    "\n",
    "We use the HCP S1200 Young Adult dataset ([van Essen et al., 2013](https://pubmed.ncbi.nlm.nih.gov/23684880/)) to fit the HMM and predict age and cognitive variables, as well as classify sex from resting-state fMRI dynamics (amplitude and functional connectivity). For reproducibility and since the dataset is very large, we provide a pretrained HMM in the example data folder. \n",
    "\n",
    "Authors: Christine Ahrends <christine.ahrends@cfin.au.dk>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. [Preparation](#preparation)\n",
    "    * [Load and prepare data](#load-data)\n",
    "    * [Train HMM](#train-hmm)\n",
    "        * [Levels of separation between training and test set](#train-test-sep)\n",
    "2. [Example: Predicting individual traits from an HMM in split data](#example-prediction)\n",
    "3. [Example: Classifying sex from an HMM in split data](#example-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation <a id=\"preparation\"></a>\n",
    "If you dont have the **GLHMM-package** installed, then run the following command in your terminal:\n",
    "\n",
    "```pip install glhmm```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**\\\n",
    "Let's start by importing the required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glhmm import glhmm, preproc, prediction, io, graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data <a id=\"load-data\"></a>\n",
    "This requires that the following files are available:\n",
    "* data: HCP rest fMRI timeseries from 1001 subjects in groupICA50 parcellation\n",
    "* behav: behavioural/demographic items from 1001 HCP subjects\n",
    "* T_t: indices for beginning and start of each subject's scanning session\n",
    "* twins: matrix indicating family structure (subjects x subjects), zeros for unrelated subjects and positive values for related subjects (diagonal will be ignored)\n",
    "* confounds: confounding variables for 1001 subjects (here sex and head motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv files and convert to numpy arrays\n",
    "data = pd.read_csv('tc1001_RESTall_groupICA50.csv', header=None).to_numpy()\n",
    "T_t = pd.read_csv('T.csv', header=None).to_numpy()\n",
    "behav = pd.read_csv('behav1001_35var.csv', header=None).to_numpy()\n",
    "twins = pd.read_csv('twins.csv', header=None).to_numpy()\n",
    "confounds = pd.read_csv('confounds.csv', header=None).to_numpy()\n",
    "\n",
    "# check that dimensions of input files are correct:\n",
    "print(data.shape) # data should be (n_subjects*n_timepoints, n_parcels)\n",
    "print(behav.shape) # behav should be (n_subjects, n_variables)\n",
    "print(T_t.shape) # T_t should be (n_subjects, 2)\n",
    "print(twins.shape) # twins should be (n_subjects, n_subjects)\n",
    "print(confounds.shape) # confounds should be (n_subjects, n_confounds) or (n_subjects,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise timeseries for all following computations. This is an important step, especially when looking at differences between individuals, to make sure that predictions are not driven by measurement noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc,_ = preproc.preprocess_data(data, T_t)\n",
    "del data # we will only use the standardised version of the time series going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training and test set\n",
    "To illustrate a split dataset, we will here simply use the first 100 subjects as training set and the next 50 subjects as a test set. In reality, these may be obtained from splitting functions, e.g. sklearn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split), or come from separate populations or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(100)\n",
    "test_indices = np.arange(100, 150)\n",
    "\n",
    "# separate time series for training and test set\n",
    "T_train = T_t[train_indices]\n",
    "T_test = T_t[test_indices]\n",
    "# to get time series indices, use aux functions:\n",
    "ts_train = glhmm.auxiliary.make_indices_from_T(T_train)\n",
    "ts_test = glhmm.auxiliary.make_indices_from_T(T_test)\n",
    "data_train = data_preproc[ts_train,:]\n",
    "data_test = data_preproc[ts_test,:]\n",
    "\n",
    "# separate target variables for training and test set\n",
    "target_train = behav[train_indices,:]\n",
    "target_test = behav[test_indices,:]\n",
    "\n",
    "# separate confounds for training and test set\n",
    "confounds_train = confounds[train_indices, :]\n",
    "confounds_test = confounds[test_indices, :]\n",
    "\n",
    "# separate group structure for training and test set \n",
    "twins_train = twins[train_indices, train_indices]\n",
    "twins_test = twins[test_indices, test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train HMM <a id=\"train-hmm\"></a>\n",
    "#### Levels of separation between training and test set <a id=\"train-test-sep\"></a>\n",
    "When working with split data, it is important to consider on which levels of data processing this divide needs to be preserved. In our case, these levels are: preprocessing (not covered here), fitting the group-level HMM, feature normalisation/deconfounding, and model training. While training and test set are always kept separate for out-of-sample prediction for the last step, the other steps may differ. The gold-standard recommendation is to keep training and test data separate on every level to avoid leakage of information. However, in cases where the purpose is mainly to explain relations in the data (rather than generalising predictions to unseen data), the divide may be more important for the target variable. By default, the toolbox functions keep training and test set separate for feature normalisation/deconfounding, and model training. Whether the group-level HMM is fit only to the training dataset or to the concatenated data of training and test set is up to the user. \n",
    "\n",
    "We will here show an example of training the HMM only on the subjects we want to later use as training set to follow the gold-standard recommendation. Note though that when using the Fisher kernel, heterogeneity between the training and test set may bias the estimation, since individual subjects' features are here defined *in reference to the group-level model*, which test subjects would then not be part of (see [Ahrends, Woolrich, & Vidaurre, 2024](https://elifesciences.org/reviewed-preprints/95125) for an in-depth exploration of the effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "hmm = glhmm.glhmm(model_beta='no', K=6, covtype='full')\n",
    "hmm.train(X=None, Y=data_train, indices=T_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can load the pre-trained HMM (which was fitted to all subjects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â hmm = io.load_hmm('./example_data/hmm_hcp_preproc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting individual traits from an HMM in split data <a id=\"example-prediction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classifying sex from an HMM In split data <a id=\"example-classification\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glhmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
